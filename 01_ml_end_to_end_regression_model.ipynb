{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_ml_end_to_end_regression_model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "srBMG6xGmjXQ",
        "CKlaaCaO9n3C",
        "-EDlwx9RrZwp",
        "xd59yo9c7Qy6",
        "wzhaH1esmYXX",
        "aU1cryeVM2r0",
        "2ieLhC6MjQI_",
        "IYy8MAegtj-D",
        "pzmxxUXay46j",
        "53mh8wqCw4Pj",
        "khfV6vDTya6B",
        "KOZ0tXpV1GLW",
        "KY7rCvKK2wd2",
        "dTqDt7tSN_iD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srBMG6xGmjXQ"
      },
      "source": [
        "# Get Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKlaaCaO9n3C"
      },
      "source": [
        "> ## Download the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIvKjaWhskSx"
      },
      "source": [
        "> Write a small function to get the data. If the data changes constantly you can call this function perodically and get the new and updated data. The `fetch_housing_data()` downloads the data, uncompresses the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN_vb7aoogzc"
      },
      "source": [
        "import os\n",
        "import tarfile\n",
        "from six.moves import urllib"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKubKnrNow-0"
      },
      "source": [
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
        "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
        "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX2gNFncpM5t"
      },
      "source": [
        "def fetch_housing_data(h_url=HOUSING_URL, h_path=HOUSING_PATH):\n",
        "    if not os.path.isdir(h_path):\n",
        "        os.makedirs(h_path)\n",
        "\n",
        "    tgz_path = os.path.join(h_path, \"housing.tgz\")\n",
        "    urllib.request.urlretrieve(h_url, tgz_path)\n",
        "    h_tgz = tarfile.open(tgz_path)\n",
        "    h_tgz.extractall(path=h_path)\n",
        "    h_tgz.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7g9BDUNqua5"
      },
      "source": [
        "fetch_housing_data()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EDlwx9RrZwp"
      },
      "source": [
        "> ## Working with **Pandas, matplotlib, train_test_split from sklearn**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWwSQGhr7bum"
      },
      "source": [
        "Here we try to achieve the following:\n",
        "* gain insights from the data through the **Pandas** library.\n",
        "* plot and gain insights from the `hist()` plots.\n",
        "* create a test set free from biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4En6dcep_cC"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_housing_data(h_path=HOUSING_PATH):\n",
        "    csv_path = os.path.join(h_path, \"housing.csv\")\n",
        "    return pd.read_csv(csv_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zFX-TYXqd8X"
      },
      "source": [
        "data = load_housing_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj4dkMQTsSS6"
      },
      "source": [
        "* `data.head()` shows the first 5 elements\n",
        "* `data.tail()` shows the last 5 elements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4US-iBvrUbx"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of8TEPL3sY0_"
      },
      "source": [
        "* `data.info()` gives the info of each column in the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vn5WegIxDbH"
      },
      "source": [
        "> From the `info` we see that only $ocean\\_proximity$ has a **non-numerical** **Dtype**, this column contains an **object**, thus this column can contain any valid python object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4PpiiCOrVvZ"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xML84P56shZQ"
      },
      "source": [
        "* `data[col].value_counts()` gives the count of each categorical data in a particular coulmn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iG1ef1Srwlb"
      },
      "source": [
        "data[\"ocean_proximity\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsTEcI7UswRY"
      },
      "source": [
        "* `data.describe()` gives some important info regarding the numerical values in the data\n",
        "\n",
        "* **Note**: The null values are ignored thus the count of $total\\_bedrooms$ is *20433* and not *20640* as there are *207* missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYYSyOvvtift"
      },
      "source": [
        "* The  **std**\n",
        "row  shows  the  *standard  deviation*,  which  measures  how  dispersed  the  values  are.\n",
        "\n",
        "* The  *25%,  50%,  and  75%*  rows  show  the  corresponding  percentiles:  a  percentile  indiâ€\n",
        "cates the value below which a given percentage of observations in a group of observaâ€\n",
        "tions  falls. **eg.** For  example,  25%  of  the  districts  have  a  housing_median_age  lower  than\n",
        "18, while 50% are lower than 29 and 75% are lower than 37. These are often called the\n",
        "25th percentile (or 1st quartile), the median, and the 75th percentile (or 3rd quartile)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciQeTaD9sKDB"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEZK6SuQsP2x"
      },
      "source": [
        "* Another way to make sense of the data is to plot the data and we can use the histogram plot for the same\n",
        "* A *histogram* shows the number of insatances (on the vertical axis) that have a given range on the horizontal axis\n",
        "\n",
        "> **Note:** You can either plot this obe attribute at a time, or you can call `hist()` method in the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj6PHzgqvawl"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "data.hist(bins=50, figsize=(20, 15))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKxu9ur1vtPK"
      },
      "source": [
        "> Creating a test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ5_rri7o1Bc"
      },
      "source": [
        "* Using the standard method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSVVmQgBo8bj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def split_train_test(data, test_ratio):\n",
        "    shuffled_indices = np.random.permutation(len(data))\n",
        "    test_set_size = int(len(data)*test_ratio)\n",
        "    test_indices = shuffled_indices[:test_set_size]\n",
        "    train_indices = shuffled_indices[test_set_size:]\n",
        "    return data.iloc[train_indices], data.iloc[test_indices]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f72q2z-mp-gz"
      },
      "source": [
        "train_set, test_set = split_train_test(data, 0.2)\n",
        "print(len(train_set), len(test_set))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rZN12T4uJ9m"
      },
      "source": [
        "* Now in the above case of spliting the data every time we run the cell we will end up having a different test and train set. eventually out model has ended up seeing the entire data. To solve this issue we can use the `np.random.seed()` before `np.random.permutation()` and we will end up getting the same data.\n",
        "* But, now say our data is constantly being updated. What happens is every time we take a spilt we end up having some previous test data in the train set and vice versa.\n",
        "* To have a stable train/test set even after updating the dataset, a common solution is to use an instance identifier to decide whether or not it should be in the test set. For example, you could compute a hash of each instance's identifier and put that instance in the tset set if the has is lower that or equal to 20% of the minimum hash value. This ensures that ths test set will remain consistant across multiple runs\n",
        "\n",
        "\n",
        "    \n",
        "        from zlip import crc32\n",
        "        def test_set_check(indentifier, test_size):\n",
        "            return crc32(np.int64(indentifier)) & 0xffffffff < test_ratio*2**32\n",
        "        \n",
        "        def spilt_train_test_by_id(data, test_ratio, id_column):\n",
        "            ids = data[column_id]\n",
        "            in_test_set = ids.apply(lambda id_:test_set_check(id_, test_ratio))\n",
        "            return data.loc[~in_test_set], data.loc[in_test_set]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfZpbuv4qLwA"
      },
      "source": [
        "> You can also use the `train_test_split()` function from the *sklearn* module imported as `from sklearn.model_selection import train_test_split()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZkH2QbXr61z"
      },
      "source": [
        "* So far we have considered purely random sampling methods. This is generally fine if\n",
        "your dataset is large enough (especially relative to the number of attributes), but if it\n",
        "is  not,  you  run  the  risk  of  introducing  a  significant  sampling  bias.\n",
        "\n",
        "* When  a  survey\n",
        "company decides to call 1,000 people to ask them a few questions, they donâ€™t just pick\n",
        "1,000  people  randomly  in  a  phone  book.  They  try  to  ensure  that  these  1,000  people\n",
        "are  representative  of  the  whole  population.  For  example,  the  US  population  is  comâ€\n",
        "posed of 51.3% female and 48.7% male, so a well-conducted survey in the US would\n",
        "try to maintain this ratio in the sample: 513 female and 487 male. This is called stratiâ€\n",
        "fied  *sampling*: the  population  is  divided  into  homogeneous  subgroups  called  *strata*,\n",
        "and the right number of instances is sampled from each stratum to guarantee that the\n",
        "test  set  is  representative  of  the  overall  population.  \n",
        "\n",
        "* If  they  used  purely  random  samâ€\n",
        "pling, there would be about 12% chance of sampling a skewed test set with either less\n",
        "than  49%  female  or  more  than  54%  female.  Either  way,  the  survey  results  would  be\n",
        "significantly biased.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0HLIXVMsv3Z"
      },
      "source": [
        "* Suppose  you  chatted  with  experts  who  told  you  that  the  median  income  is  a  very\n",
        "important  attribute  to  predict  median  housing  prices.  You  may  want  to  ensure  that\n",
        "the test set is representative of the various categories of incomes in the whole dataset.\n",
        "\n",
        "> Let's look at the histogram of the meadian income"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzsB_G5ZsaXr"
      },
      "source": [
        "* The  following  code  uses  the\n",
        "[pd.cut()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html?highlight=cut#pandas.cut) function  to  create  an  income  category  attribute  with  5  categories  (labeled\n",
        "from 1 to 5): category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from 1.5 to 3, and so on:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euT50oumsroW"
      },
      "source": [
        "data[\"income_cat\"] = pd.cut(data[\"median_income\"],\n",
        "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "                               labels=[1, 2, 3, 4, 5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndmo8kQm22cj"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auauxIXAsupk"
      },
      "source": [
        "data[\"income_cat\"].hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyEHIqTts1RV"
      },
      "source": [
        "* Now  you  are  ready  to  do  stratified  sampling  based  on  the  income  category.  For  this\n",
        "you can use Scikit-Learnâ€™s `StratifiedShuffleSplit` class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgLJ8p-MtGNc"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state=42)\n",
        "\n",
        "for train_idx, test_idx in split.split(data, data[\"income_cat\"]):\n",
        "    strat_train_set = data.iloc[train_idx]\n",
        "    strat_test_set = data.iloc[test_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iESD4qyyttKR"
      },
      "source": [
        "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OcjTeNPuGnj"
      },
      "source": [
        "> Now remove the `income_cat` attribute "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dpz4DfGX7Bfx"
      },
      "source": [
        "for set_ in (strat_train_set, strat_test_set):\n",
        "    set_.drop(\"income_cat\", axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xd59yo9c7Qy6"
      },
      "source": [
        "# Visialize the data to gain insights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJL9HaSI9d94"
      },
      "source": [
        "> Make a copy of the train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh7Vdj_evvUu"
      },
      "source": [
        "visual_cp = strat_train_set.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxAA37QMEorI"
      },
      "source": [
        "[visual_cp.plot()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.plot.html?highlight=plot#pandas.Series.plot)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls_r0Q16v-9u"
      },
      "source": [
        "visual_cp.plot(kind='scatter', x='longitude', y='latitude')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gjNUgXRwJxu"
      },
      "source": [
        "> Set alpha patameter to 0.1 for better visualization\n",
        "Setting alpha to 0.1 helps visualize place where there is high density of data points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp-9FZwMwWDG"
      },
      "source": [
        "visual_cp.plot(kind='scatter', x = 'longitude', y='latitude', alpha=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1RaeLNzwgsV"
      },
      "source": [
        "> Plot the median_house_price\n",
        "\n",
        "* The radius of each circle represents\n",
        "the districtâ€™s population (option s), and the color represents the price (option c). We\n",
        "will  use  a  predefined  color  map  (option  cmap)  called  jet,  which  ranges  from  blue\n",
        "(low values) to red (high prices)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R53zKUl3xIre"
      },
      "source": [
        "visual_cp.plot(kind='scatter', x='longitude', y = 'latitude', alpha=0.4, s=visual_cp[\"population\"]/100, label='population', figsize=(10,7),c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cYNw8r5Gjfa"
      },
      "source": [
        "The plot tells us that the housing prices are very much related to the location (eg. close to the ocean) and tp the population density"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEC724eixuFd"
      },
      "source": [
        "> ## Looking for correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhQXB8ftNhUQ"
      },
      "source": [
        "* [Covariance in Statistics](https://www.youtube.com/watch?v=mlxwAPTjiCA)\n",
        "* [Statistics- What is Pearson Correlation Coefficient? Difference between Correlation and Covariance](https://www.youtube.com/watch?v=6fUYt1alA1U)\n",
        "* [Spearman's rank correlation coefficient- Statistics](https://www.youtube.com/watch?v=CIQ3u7CvOEU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMfWDtMdelNQ"
      },
      "source": [
        "> you can easily compute the *standard correlation coefficient* also called *Pearson's r* between every pair using the `corr()` method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfmIhnAOfAG5"
      },
      "source": [
        "corr_matrix = visual_cp.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udnbYCFZfHCi"
      },
      "source": [
        "corr_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xBtoMmNfJDB"
      },
      "source": [
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fOoNoE6fStC"
      },
      "source": [
        "* The correlation coefficient ranges from -1 to 1\n",
        "* When it is close to 1, it means that there is a strong positive correlation; *for eg*, *meadian house* value tends to go up when the median income goes up.\n",
        "* When the coefficient is close to -1, it means that there is a strong negative correlation;\n",
        "you see a strong negative correlation between *latitude* and the meadian of the house value.\n",
        "* Finally the coefficient close to 0 mean that there os no linear correlation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pchPNSTXgoTb"
      },
      "source": [
        "> **ðŸš€Note:** The correlation coefficient only measures linear correlations(*if x goes up, then generally y goes up/down*). It may completely miss out on non-linear relationships. For example, your height in inches has a correlaâ€\n",
        "tion coefficient of 1 with your height in feet or in nanometers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV1HO8Vnhv-n"
      },
      "source": [
        "> ðŸ¤– **Another way** to check for correlations between attributes is to use Pandas method `scatter_matrix`, which plots every numerical attribute against every other numerical attribute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruWITg8mkQre"
      },
      "source": [
        "> **ðŸš€Note:** Since there are now 11 numerical attributes, you would get 11<sup>2</sup> =\n",
        "121  plots\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eciQul7qjiDF"
      },
      "source": [
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
        "scatter_matrix(visual_cp[attributes], figsize=(12,8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsL8lozBj-U7"
      },
      "source": [
        "> The main diagonal (top left to bottom right) would be full of straight lines if Pandas\n",
        "plotted each variable against itself, which would not be very useful. So instead Pandas\n",
        "displays a histogram of each attribute (other options are available; see Pandasâ€™ docuâ€\n",
        "mentation for more details)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E7rdf_PlP5V"
      },
      "source": [
        "visual_cp.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgJc3LvDlaV9"
      },
      "source": [
        "> ðŸ’¡ This  plot  reveals  a  few  things.  First,  the  correlation  is  indeed  very  strong;  you  can\n",
        "clearly  see  the  upward  trend  and  the  points  are  not  too  dispersed.  Second,  the  price\n",
        "cap  that  we  noticed  earlier  is  clearly  visible  as  a  horizontal  line  at  \\$500,000.  But  this\n",
        "plot  reveals  other  less  obvious  straight  lines:  a  horizontal  line  around  \\$450,000,\n",
        "another around \\$350,000, perhaps one around \\$280,000, and a few more below that.\n",
        "You may want to try removing the corresponding districts to prevent your algorithms\n",
        "from learning to reproduce these data quirks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27kS0s6zTYIi"
      },
      "source": [
        "The total number of romms in a district is not very useful if you don't know how many households there are. similarly number of bedrooms by itself is not useful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIiy-dPVShAu"
      },
      "source": [
        "visual_cp[\"rooms_per_household\"] = visual_cp[\"total_rooms\"]/visual_cp[\"households\"]\n",
        "visual_cp[\"bedrooms_per_room\"] = visual_cp[\"total_bedrooms\"]/visual_cp[\"total_rooms\"]\n",
        "visual_cp[\"population_per_household\"] = visual_cp[\"population\"]/visual_cp[\"households\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7Yo5gxlTCo4"
      },
      "source": [
        "corr_mat_2 = visual_cp.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz3WScEJTF_3"
      },
      "source": [
        "corr_mat_2[\"median_house_value\"].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzhaH1esmYXX"
      },
      "source": [
        "# Preparing data for Machine Learning Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOSnZdAwMKYO"
      },
      "source": [
        "> ðŸ§ But first letâ€™s revert to a clean training set (by copying strat_train_set once again),\n",
        "and letâ€™s separate the predictors and the labels since we donâ€™t necessarily want to apply\n",
        "the  same  transformations  to  the  predictors  and  the  target  values  (note  that  drop()  \n",
        "creates a copy of the data and does not affect strat_train_set)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AookJYVkMgZ8"
      },
      "source": [
        "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
        "housing_labels = strat_train_set[\"median_house_value\"].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU1cryeVM2r0"
      },
      "source": [
        "> ### Handling missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQzVB_bHNC2V"
      },
      "source": [
        "> ðŸ¤– You have three options to fix missing data:\n",
        "* Get rid of the corresponding districts. `housing.dropna(subset=[\"total_bedrooms\"])`\n",
        "* Get rid of the whole arribute. `housing.drop(\"total_bedrooms\", axis=1)`\n",
        "* Set the values to some value(zero, the mean, the median, etc)\n",
        "        median = housing[\"total_bedrooms\"].median()\n",
        "        housing[\"total_bedrooms\"].fillna(median, inplace=True)\n",
        "if you choose option 3 then save the median and for now update only the test set and later of used the saved median to update the missing values in the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzyxetVvM_jD"
      },
      "source": [
        "handling missing data using sklearns `SimpleImputer` class.\n",
        "Using the Simple Imputer we can apply any startegy we want like, mean, median, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbeaFIrpguY2"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy='median')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WGcTP43hIOI"
      },
      "source": [
        "# removing ocean_proximity because SimpleImputer can be used only with numerical data\n",
        "housing_num = housing.drop(\"ocean_proximity\", axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JthF9KCghigG"
      },
      "source": [
        "imputer.fit(housing_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnGiNcJ1hnsd"
      },
      "source": [
        "The imputer has simply computed the median of each attribute and stored the result in its `statistics_` instance variable. The `imputer.fit()` computes the `strategy` for all the attributes in the data and replaces any missing value with the corresponding `strategy`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J-fTg_whzvl"
      },
      "source": [
        "imputer.statistics_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggb2J3SiWz8"
      },
      "source": [
        "housing_num.median()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVpd3ikaigMm"
      },
      "source": [
        "#transform the data\n",
        "X = imputer.transform(housing_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRY1nyN3ivev"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvcnABf2iwSA"
      },
      "source": [
        "#convert X into pd dataframe\n",
        "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgq9JkTUi_Zh"
      },
      "source": [
        "housing_tr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ieLhC6MjQI_"
      },
      "source": [
        "### Handling text and categorical values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYrau2e9pPKo"
      },
      "source": [
        "housing_cat = housing[[\"ocean_proximity\"]]\n",
        "housing_cat.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80itTxKaqybb"
      },
      "source": [
        "we use sklearn's `OrdinalEncoder` to convert the categorical text data into numerical equivalent numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "345tGmKqrGS3"
      },
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "ordial_encoder = OrdinalEncoder()\n",
        "housing_cat_encoded = ordial_encoder.fit_transform(housing_cat)\n",
        "housing_cat_encoded[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHJa2zMXrbjV"
      },
      "source": [
        "to see all the categories that were present us `categories`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwOXHRd3rsOi"
      },
      "source": [
        "ordial_encoder.categories_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsRmXm_Drvq0"
      },
      "source": [
        "> One hot Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1usjQ2TLr1xH"
      },
      "source": [
        "to perform one-hot encoding use the `OneHotEncoder` class from sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpWjL5nGsEDn"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "cat_encoder = OneHotEncoder()\n",
        "housing_1_cat = cat_encoder.fit_transform(housing_cat)\n",
        "housing_1_cat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dOkbl1hsUvh"
      },
      "source": [
        "> ðŸ’¡ Notice that the output is scipy sparse matrix, instead of Numpy array. This is useful if there are 1000 categories full of 1's and 0's except for a single 1 per row, it is wasteful of memory. This this shows only the places where 1 is stored. But if you want a 2D numpy array then use the `toarray()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkGoo_1HtRUK"
      },
      "source": [
        "housing_1_cat.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg6TbJmKtUMY"
      },
      "source": [
        "You can also pass text value for this encoder and you can see all the categories in the `categories_` attribute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXe9rlahtgQl"
      },
      "source": [
        "cat_encoder.categories_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYy8MAegtj-D"
      },
      "source": [
        "###Custom Transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ryN3BnovsZG"
      },
      "source": [
        "You need to create a class and implement 3 methods `fit() returning self`, `transform()`, `fit_transform()` you can get rid fo the last one if you add `TransformerMixin` for the base class. If you add `BaseEstimator` as a base class and avoid $*agrs$ and $**kargs$ in your custom constructor then you will get two additional methods `get_params()` and `set_params()` which will be useful for hyperparameter tuning. The following custom transformer adds the combined attributes discussed earlier,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2yn0ACawCxQ"
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# column index\n",
        "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
        "\n",
        "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n",
        "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
        "    def fit(self, X, y=None):\n",
        "        return self  # nothing else to do\n",
        "    def transform(self, X):\n",
        "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
        "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
        "        if self.add_bedrooms_per_room:\n",
        "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
        "            return np.c_[X, rooms_per_household, population_per_household,\n",
        "                         bedrooms_per_room]\n",
        "        else:\n",
        "            return np.c_[X, rooms_per_household, population_per_household]\n",
        "\n",
        "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
        "housing_extra_attribs = attr_adder.transform(housing.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzmxxUXay46j"
      },
      "source": [
        "### Building Transformation pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_dVSswKuTtu"
      },
      "source": [
        "We can build a transformation pipeline using sklearn's `Pipeline` class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw4AuKk0uc4P"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "        ('attribs_adder', CombinedAttributesAdder()),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])\n",
        "\n",
        "housing_num_tr = num_pipeline.fit_transform(housing_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05jnD6tpvKrA"
      },
      "source": [
        "housing_num_tr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP5OFWxqvsdp"
      },
      "source": [
        "The Pipeline constructor takes a list of name/estimator pairs defining the sequence of steps. All but last estimator must be transformers(i.e. they must have the `fit_transform()` method)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN6WbUezw-AV"
      },
      "source": [
        "Sklearn's ColumnTransformer class is used to apply these transforms to each and every transforms that are available in the same order.\n",
        "\n",
        "In the constructor it takes a list of tuples, where each tuple contains a name a transformer, and a list of names of columns that the transformer should be applied to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DXTnwCBwIYN"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "num_attribs = list(housing_num)\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "                                   (\"num\", num_pipeline, num_attribs),\n",
        "                                   (\"cat\", OneHotEncoder(), cat_attribs)\n",
        "])\n",
        "\n",
        "housing_prepared = full_pipeline.fit_transform(housing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_ckfBHpwQLZ"
      },
      "source": [
        "num_attribs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vvJGgOXwRh2"
      },
      "source": [
        "housing_prepared"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53mh8wqCw4Pj"
      },
      "source": [
        "# Select and train a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khfV6vDTya6B"
      },
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwvl6H_SzVU7"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(housing_prepared, housing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYBU1-0wzlep"
      },
      "source": [
        "some_data = housing.iloc[:5]\n",
        "some_labels = housing_labels.iloc[:5]\n",
        "some_data_prepared = full_pipeline.transform(some_data)\n",
        "print(\"Predictions: \", lin_reg.predict(some_data_prepared))\n",
        "print(\"labels: \", list(some_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUqN3lKC0OUA"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "housing_predictions = lin_reg.predict(housing_prepared)\n",
        "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "lin_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOZ0tXpV1GLW"
      },
      "source": [
        "## Decision Trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u191aT5s1zZd"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg = DecisionTreeRegressor()\n",
        "tree_reg.fit(housing_prepared, housing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHZTjFfd2GNW"
      },
      "source": [
        "housing_predictions = tree_reg.predict(housing_prepared)\n",
        "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "tree_rmse = np.sqrt(tree_mse)\n",
        "tree_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD5EI1r_2cqa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY7rCvKK2wd2"
      },
      "source": [
        "##Better Evaluation Using Cross-Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UEWxwkI24sg"
      },
      "source": [
        "You should not touch the test data until you are ready for production instead you should use a cross validation test for better evaluation.\n",
        "\n",
        "A great alternative is to use sklearns k-fold cross-validation feature. The following code randomly splits the training set into 10 subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evalutaion every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores\n",
        "\n",
        "> ðŸ’¡ Note: Sklearn's cross-validation feature expects a **utility function (grater the better)** rather than a **cost function (lower the better)**, so the scoring function is actually the opposite of the MSE (i.e. a negative value), which is why the preceding code computes `-scores` before calcaulting the root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQI_qo823GdQ"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
        "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
        "tree_rmse_scores = np.sqrt(-scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTqUB_KS4hBz"
      },
      "source": [
        "def display_scores(scores):\n",
        "    print(\"Scores:\", scores)\n",
        "    print(\"Mean:\", scores.mean())\n",
        "    print(\"Standard deviation:\", scores.std())\n",
        "\n",
        "display_scores(tree_rmse_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq-ZS3HU5v6l"
      },
      "source": [
        "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv=10)\n",
        "lin_rmse_scores = np.sqrt(-lin_scores)\n",
        "display_scores(lin_rmse_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRhxiIvcNndj"
      },
      "source": [
        "From the above scores we can conclude that the decision tree is overfitting the model badly and the performance of linear regression is better than decision tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTqDt7tSN_iD"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlVRLO6hOWWj"
      },
      "source": [
        "Random Forest works by training many decision trees in random subset of the features, the averaging out there predictions. Building a model on top of many other models is called *Ensemble Learning*, and it is often a great way to push ML algos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJsTIwigOReh"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "forest_reg = RandomForestRegressor()\n",
        "forest_reg.fit(housing_prepared, housing_labels)\n",
        "\n",
        "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv=10)\n",
        "forest_rmse_scores = np.sqrt(-forest_scores)\n",
        "\n",
        "housing_predictions = forest_reg.predict(housing_prepared)\n",
        "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "forest_rmse = np.sqrt(forest_mse)\n",
        "print('rmse: ', forest_rmse)\n",
        "display_scores(forest_rmse_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAgc0cF7Pvtd"
      },
      "source": [
        "# Fine Tune Your models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMCxic6WW-6X"
      },
      "source": [
        "One option to change hyperparameter tuning is to change them mannually, until you find a good combination. but its too time comsuming.\n",
        "\n",
        "Instead using sklearn `GridSearchCV`. All you need to do is tell which hyperparemeters you wnat it to experiment with and what values to try out, and it will use cross-validation to evaluate all the possible combination of hyperparameter values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfe5jeQBU1fP"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = [\n",
        "              {'n_estimators':[3,10,30], 'max_features':[2,4,6,8]},\n",
        "              {'bootstrap':[False], 'n_estimators':[3,10], 'max_features':[2,3,4]}\n",
        "]\n",
        "\n",
        "forest_reg = RandomForestRegressor()\n",
        "\n",
        "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error',\n",
        "                           return_train_score=True)\n",
        "\n",
        "grid_search.fit(housing_prepared, housing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaL8-k8NcSPm"
      },
      "source": [
        "This `param_grid` tells sklearn to first evaluate all the 3x4=12 combinations of `n_estimators` and `max_features` hyperparameter values specified in the first `dict`, then try all 2x3=6combinations of hyperparameter values in the secont `dict`, but this time with the `bootstrap` set to `False` which is `True` by default.\n",
        "\n",
        "The gird search will explore 12+6=18 combinations of `RandomForestRegresoor`, it will train the model 5 times (since we are using five-fold cross validation). Meaning it will train 18x5=90 rounds of training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7zYWHprdoi3"
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wM3WVHGeKkC"
      },
      "source": [
        "grid_search.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJtg1IfgelKm"
      },
      "source": [
        ">ðŸ’¡ Note: If `GridSeachCV` is initialized with `refit=True` (which is the default), the once it finds the best estimator using coress-validation, it retrains it in the whole training set. This is usually a good ideam since feeding it more data will likely imporve its performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yav6yiUEfnZF"
      },
      "source": [
        "cvres = grid_search.cv_results_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmW-eUJdftX5"
      },
      "source": [
        "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
        "    print(np.sqrt(-mean_score), params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WAYf2bnf7Ga"
      },
      "source": [
        "Similar to `GridSearchCV` you can use `RandomizedSearchCv`, but instead of trying out all possible combinations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration."
      ]
    }
  ]
}